{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":11254922,"datasetId":7032548,"databundleVersionId":11669857},{"sourceType":"modelInstanceVersion","sourceId":338567,"databundleVersionId":11847418,"modelInstanceId":283098}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass FF(nn.Module):\n    def __init__(self, embd_dim):\n        super().__init__()\n        self.linear1 = nn.Linear(embd_dim, 8 * embd_dim)\n        self.linear2 = nn.Linear(8 * embd_dim, embd_dim)\n        self.gelu = nn.GELU()\n\n    def forward(self, x):\n        return self.linear2(self.gelu(self.linear1(x)))","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:00:29.394036Z","iopub.execute_input":"2025-04-15T04:00:29.394319Z","iopub.status.idle":"2025-04-15T04:00:34.490807Z","shell.execute_reply.started":"2025-04-15T04:00:29.394277Z","shell.execute_reply":"2025-04-15T04:00:34.489885Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, embd_dim):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim=embd_dim, num_heads=num_heads, batch_first=True)\n\n    def forward(self, q, k, v):\n        attn_output, _ = self.attn(q, k, v)\n        return attn_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:06:22.383768Z","iopub.execute_input":"2025-04-15T04:06:22.384073Z","iopub.status.idle":"2025-04-15T04:06:22.389471Z","shell.execute_reply.started":"2025-04-15T04:06:22.384052Z","shell.execute_reply":"2025-04-15T04:06:22.388668Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass Decode(nn.Module):\n    def __init__(self, num_heads, embd_dim):\n        super().__init__()\n        self.attn = MultiHeadAttention(num_heads, embd_dim)\n        self.norm1 = nn.LayerNorm(embd_dim)\n        self.norm2 = nn.LayerNorm(embd_dim)\n        self.ff = FF(embd_dim)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dropout2 = nn.Dropout(0.2)\n\n    def forward(self, x):\n        x_norm = self.norm1(x)\n        attn_out = self.attn(x_norm, x_norm, x_norm)\n        x = x + self.dropout1(attn_out)\n        x_norm = self.norm2(x)\n        x = x + self.dropout2(self.ff(x_norm))\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:06:22.591788Z","iopub.execute_input":"2025-04-15T04:06:22.592083Z","iopub.status.idle":"2025-04-15T04:06:22.598269Z","shell.execute_reply.started":"2025-04-15T04:06:22.592061Z","shell.execute_reply":"2025-04-15T04:06:22.597395Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        seq_length,\n        num_layers,\n        num_heads,\n        embd_dim,\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embd_dim)\n        self.pos_embedding = nn.Embedding(seq_length, embd_dim)\n        self.layers = nn.ModuleList(\n            [Decode(num_heads, embd_dim) for i in range(num_layers)]\n        )\n        self.norm = nn.LayerNorm(embd_dim)\n\n    def forward(self, x):\n        seq_length = x.size(1)\n        positions = (\n            torch.arange(0, seq_length, device=x.device).unsqueeze(0).expand_as(x)\n        )\n        x1 = self.embedding(x) + self.pos_embedding(positions)\n        for layer in self.layers:\n            x1 = layer(x1)\n        return self.norm(x1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:06:22.791175Z","iopub.execute_input":"2025-04-15T04:06:22.792139Z","iopub.status.idle":"2025-04-15T04:06:22.798262Z","shell.execute_reply.started":"2025-04-15T04:06:22.792109Z","shell.execute_reply":"2025-04-15T04:06:22.797561Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class GPT3(nn.Module):\n    def __init__(self, vocab_size, seq_length, num_heads, num_layers, embd_dim):\n        super().__init__()\n        self.dec = Decoder(vocab_size, seq_length, num_heads, num_layers, embd_dim)\n        self.out = nn.Linear(embd_dim, vocab_size)\n        self.seq_length = seq_length\n        self.vocab_size = vocab_size\n\n    def forward(self, x):\n        x = self.dec(x)\n        x = self.out(x)\n        return x\n\n    def generate(self, input_ids, max_length=50, temperature=0.9):\n        self.eval()\n        output = input_ids.tolist()[0]\n        with torch.no_grad():\n            for _ in range(max_length):\n                input_ids = input_ids.to(\"cuda\")\n                logits = self(input_ids)\n                logits = logits[:, -1, :] / temperature\n                probs = nn.functional.softmax(logits, dim=-1)\n                next_token = torch.multinomial(probs, num_samples=1)\n                output.append(int(next_token[0, 0]))\n                input_ids = torch.cat([input_ids[:, 1:], next_token], dim=1)\n        self.train()\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:06:23.065121Z","iopub.execute_input":"2025-04-15T04:06:23.065908Z","iopub.status.idle":"2025-04-15T04:06:23.072905Z","shell.execute_reply.started":"2025-04-15T04:06:23.065878Z","shell.execute_reply":"2025-04-15T04:06:23.071955Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"vocab_size = 50000 # dont touch this\nseq_length = 128 # length of the input sequence, gpt3 uses 1024, use a smaller value for gpu constraints\nnum_heads = 12 # number of attention heads, gpt3 uses 96, use a smaller value such that embd dim is divisible by num_heads\nnum_layers = 12 # number of transformer blocks, gpt3 uses 96, use a smaller value \nembd_dim = 768 # embedding dimension, gpt3 uses 12288, use a smaller value\nmodel = GPT3(vocab_size, seq_length, num_heads, num_layers, embd_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:06:23.273718Z","iopub.execute_input":"2025-04-15T04:06:23.274013Z","iopub.status.idle":"2025-04-15T04:06:25.491339Z","shell.execute_reply.started":"2025-04-15T04:06:23.273993Z","shell.execute_reply":"2025-04-15T04:06:25.490713Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"/kaggle/input/gpt3/pytorch/default/1/model.pth\", weights_only=True, map_location=\"cpu\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:06:26.917195Z","iopub.execute_input":"2025-04-15T04:06:26.917943Z","iopub.status.idle":"2025-04-15T04:06:27.886050Z","shell.execute_reply.started":"2025-04-15T04:06:26.917913Z","shell.execute_reply":"2025-04-15T04:06:27.885351Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ntotal_params = count_parameters(model)\nprint(f\"Total number of parameters: {total_params:,}\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of parameters: 246,975,824\n"]}],"execution_count":null}]}