{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 11254922,
          "sourceType": "datasetVersion",
          "datasetId": 7032548
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download('shusrith/wikipedia-data-bpe')\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "id": "J3bYiTcBq9iX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b21675d-e3f9-4960-f73b-396aeeb13ec9"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/shusrith/wikipedia-data-bpe?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 1.77G/2.65G [01:28<00:44, 21.2MB/s]"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "MwDudaubsUj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from datasets import load_dataset\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    vocab_size = 50000\n",
        "    seq_length = 128\n",
        "    num_heads = 12\n",
        "    num_layers = 12\n",
        "    embd_dim = 768\n",
        "    batch_size = 32\n",
        "    learning_rate = 6e-4\n",
        "    epochs = 10\n",
        "    eval_interval = 10000\n",
        "    eval_iters = 200\n",
        "    warmup_iters = 2000\n",
        "    min_lr = 6e-5\n",
        "    grad_clip = 1.0\n",
        "    weight_decay = 0.1\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.95\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    checkpoint_dir = '.'\n",
        "    dataset_path = f'{path}/train'"
      ],
      "metadata": {
        "trusted": true,
        "id": "xEEMAZD-q9id"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(Config.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FF(nn.Module):\n",
        "    def __init__(self, embd_dim):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(embd_dim, 8 * embd_dim)\n",
        "        self.linear2 = nn.Linear(8 * embd_dim, embd_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.gelu(self.linear1(x)))\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, embd_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim=embd_dim,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, _ = x.size()\n",
        "        attn_mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
        "        attn_output, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
        "        return attn_output\n",
        "\n",
        "\n",
        "class Decode(nn.Module):\n",
        "    def __init__(self, num_heads, embd_dim):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(num_heads, embd_dim)\n",
        "        self.norm1 = nn.LayerNorm(embd_dim)\n",
        "        self.norm2 = nn.LayerNorm(embd_dim)\n",
        "        self.ff = FF(embd_dim)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm = self.norm1(x)\n",
        "        x = x + self.dropout1(self.attn(x_norm))\n",
        "        x_norm = self.norm2(x)\n",
        "        x = x + self.dropout2(self.ff(x_norm))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        seq_length,\n",
        "        num_layers,\n",
        "        num_heads,\n",
        "        embd_dim,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embd_dim)\n",
        "        self.pos_embedding = nn.Embedding(seq_length, embd_dim)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [Decode(num_heads, embd_dim) for i in range(num_layers)]\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(embd_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_length = x.size(1)\n",
        "        positions = (\n",
        "            torch.arange(0, seq_length, device=x.device).unsqueeze(0).expand_as(x)\n",
        "        )\n",
        "        x1 = self.embedding(x) + self.pos_embedding(positions)\n",
        "        for layer in self.layers:\n",
        "            x1 = layer(x1)\n",
        "        return self.norm(x1)\n",
        "\n",
        "class GPT3(nn.Module):\n",
        "    def __init__(self, vocab_size, seq_length, num_heads, num_layers, embd_dim):\n",
        "        super().__init__()\n",
        "        self.dec = Decoder(vocab_size, seq_length, num_heads, num_layers, embd_dim)\n",
        "        self.out = nn.Linear(embd_dim, vocab_size)\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dec(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "    def generate(self, input_ids, max_length=50, temperature=0.9):\n",
        "        self.eval()\n",
        "        output = input_ids.tolist()[0]\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                input_ids = input_ids.to(\"cuda\")\n",
        "                logits = self(input_ids)\n",
        "                logits = logits[:, -1, :] / temperature\n",
        "                probs = nn.functional.softmax(logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "                output.append(int(next_token[0, 0]))\n",
        "                input_ids = torch.cat([input_ids[:, 1:], next_token], dim=1)\n",
        "        self.train()\n",
        "        return output"
      ],
      "metadata": {
        "trusted": true,
        "id": "-IFHXef3q9if"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, tokenized_data_path, seq_length):\n",
        "        self.seq_length = seq_length\n",
        "        self.data = []\n",
        "\n",
        "        print(f\"Loading data from {tokenized_data_path}...\")\n",
        "\n",
        "        # Get all .npz files (with more flexible naming)\n",
        "        npz_files = [f for f in os.listdir(tokenized_data_path) if f.endswith('.npz')][:10]\n",
        "        if not npz_files:\n",
        "            raise ValueError(\"No .npz files found in directory\")\n",
        "\n",
        "        # Load files in arbitrary order (removed problematic sorting)\n",
        "        for filename in npz_files:\n",
        "            filepath = os.path.join(tokenized_data_path, filename)\n",
        "            try:\n",
        "                with np.load(filepath) as f:\n",
        "                    arr = f['batch_arrays']\n",
        "                    if len(arr) > 0:\n",
        "                        self.data.append(arr)\n",
        "                    else:\n",
        "                        print(f\"Skipping empty array in {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {filename}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not self.data:\n",
        "            raise ValueError(\"No valid data loaded - all files were empty or corrupted\")\n",
        "\n",
        "        # Concatenate all arrays\n",
        "        try:\n",
        "            self.data = np.concatenate(self.data)\n",
        "            print(f\"\\nTotal tokens loaded: {len(self.data):,}\")\n",
        "        except ValueError as e:\n",
        "            raise ValueError(f\"Error concatenating arrays: {e}\")\n",
        "\n",
        "        # Calculate available sequences\n",
        "        self.total_sequences = (len(self.data) - 1) // self.seq_length\n",
        "        if self.total_sequences <= 0:\n",
        "            raise ValueError(\n",
        "                f\"Not enough data for seq_length={seq_length}. \"\n",
        "                f\"Need ≥{seq_length+1} tokens, got {len(self.data)}\"\n",
        "            )\n",
        "        print(f\"Available sequences: {self.total_sequences:,}\\n\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.seq_length\n",
        "        end = start + self.seq_length + 1  # +1 for target\n",
        "\n",
        "        if end > len(self.data):\n",
        "            raise IndexError(f\"Sequence {idx} out of range\")\n",
        "\n",
        "        chunk = self.data[start:end]\n",
        "        return (\n",
        "            torch.tensor(chunk[:-1], dtype=torch.long),  # input\n",
        "            torch.tensor(chunk[1:], dtype=torch.long)   # target\n",
        "        )"
      ],
      "metadata": {
        "trusted": true,
        "id": "R6kN2-OYq9ii"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    f\"{path}/vocab/vocab.json\",\n",
        "    f\"{path}/vocab/merges.txt\"\n",
        ")\n",
        "\n",
        "# Load dataset\n",
        "full_dataset = GPTDataset(Config.dataset_path, Config.seq_length)\n",
        "\n",
        "# Split into train, val, test\n",
        "train_size = int(0.9 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(\n",
        "    full_dataset, [train_size, val_size]\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=Config.batch_size, shuffle=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=Config.batch_size, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "RJZHEU16q9im"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT3(\n",
        "    vocab_size=Config.vocab_size,\n",
        "    seq_length=Config.seq_length,\n",
        "    num_heads=Config.num_heads,\n",
        "    num_layers=Config.num_layers,\n",
        "    embd_dim=Config.embd_dim\n",
        ").to(Config.device)"
      ],
      "metadata": {
        "id": "JR6Rz5lxrSzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"Total number of parameters: {total_params:,}\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=Config.learning_rate,\n",
        "    weight_decay=Config.weight_decay,\n",
        "    betas=(Config.beta1, Config.beta2)\n",
        ")"
      ],
      "metadata": {
        "id": "DCCY1tYgrUku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(it):\n",
        "    if it < Config.warmup_iters:\n",
        "        return Config.learning_rate * it / Config.warmup_iters\n",
        "    if it > Config.warmup_iters:\n",
        "        return max(\n",
        "            Config.min_lr,\n",
        "            Config.learning_rate * (0.1 ** ((it - Config.warmup_iters) / (Config.warmup_iters * 10)))\n",
        "        )\n",
        "    decay_ratio = (it - Config.warmup_iters) / (Config.warmup_iters * 9)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return Config.min_lr + coeff * (Config.learning_rate - Config.min_lr)"
      ],
      "metadata": {
        "id": "yclgizNArWqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.amp import autocast\n",
        "\n",
        "def train_step(batch):\n",
        "    x, y = batch\n",
        "    x, y = x.to(Config.device), y.to(Config.device)\n",
        "\n",
        "    with autocast(\"cuda\"):\n",
        "        logits = model(x)\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B*T, C)\n",
        "        y = y.view(B*T)\n",
        "        loss = nn.functional.cross_entropy(logits, y)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct = (preds == y).sum().item()\n",
        "        total = y.size(0)\n",
        "        accuracy = correct / total\n",
        "\n",
        "    return loss, accuracy"
      ],
      "metadata": {
        "id": "FI3AaQb6rZ70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_step(batch):\n",
        "    x, y = batch\n",
        "    x, y = x.to(Config.device), y.to(Config.device)\n",
        "\n",
        "    logits = model(x)\n",
        "    B, T, C = logits.shape\n",
        "    logits = logits.view(B * T, C)\n",
        "    y = y.view(B * T)\n",
        "\n",
        "    loss = nn.functional.cross_entropy(logits, y)\n",
        "\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    correct = (preds == y).sum().item()\n",
        "    accuracy = correct / y.size(0)\n",
        "\n",
        "    return loss, accuracy"
      ],
      "metadata": {
        "id": "n8WXPKBYAfBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    pbar = tqdm(val_loader, desc=\"Evaluating\", leave=False)\n",
        "\n",
        "    for batch in pbar:\n",
        "        loss, accuracy = eval_step(batch)\n",
        "        losses.append(loss.item())\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': f\"{loss.item():.4f}\",\n",
        "            'accuracy': f\"{accuracy:.4f}\"\n",
        "        })\n",
        "\n",
        "    return np.mean(losses), np.mean(accuracies)"
      ],
      "metadata": {
        "id": "CFJ-I894rdLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.amp import GradScaler\n",
        "\n",
        "def train():\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    scaler = GradScaler(\"cuda\")\n",
        "\n",
        "    config_dict = {\n",
        "        k: v for k, v in vars(Config).items()\n",
        "        if not k.startswith('__') and not callable(v)\n",
        "    }\n",
        "\n",
        "    for epoch in range(Config.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{Config.epochs}\")\n",
        "\n",
        "        for it, batch in enumerate(pbar):\n",
        "            lr = get_lr(it + epoch * len(train_loader))\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "            loss, accuracy = train_step(batch)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), Config.grad_clip)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': loss.item(),\n",
        "                'accuracy': accuracy,\n",
        "                'lr': lr\n",
        "            })\n",
        "\n",
        "            if (it + 1) % Config.eval_interval == 0 or it == len(train_loader) - 1:\n",
        "                val_loss, val_accuracy = evaluate()\n",
        "\n",
        "                print(f\"\\nStep {it}:\")\n",
        "                print(f\"Train Loss: {loss.item():.4f} | Train Acc: {accuracy:.4f}\")\n",
        "                print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    checkpoint = {\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'epoch': epoch,\n",
        "                        'best_val_loss': best_val_loss,\n",
        "                        'config': config_dict,\n",
        "                    }\n",
        "                    torch.save(\n",
        "                        checkpoint,\n",
        "                        os.path.join(Config.checkpoint_dir, 'best_model.pth'),\n",
        "                        pickle_protocol=4\n",
        "                    )\n",
        "                    print(\"Model saved!\")\n",
        "\n",
        "                model.train()\n",
        "\n",
        "        # Save at end of epoch\n",
        "        epoch_checkpoint = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'best_val_loss': best_val_loss,\n",
        "            'config': config_dict,\n",
        "        }\n",
        "        torch.save(\n",
        "            epoch_checkpoint,\n",
        "            os.path.join(Config.checkpoint_dir, f'epoch_{epoch}.pth'),\n",
        "            pickle_protocol=4\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-03T17:40:57.273372Z",
          "iopub.execute_input": "2025-04-03T17:40:57.273681Z",
          "iopub.status.idle": "2025-04-03T17:41:01.47096Z",
          "shell.execute_reply.started": "2025-04-03T17:40:57.273659Z",
          "shell.execute_reply": "2025-04-03T17:41:01.469821Z"
        },
        "id": "8ouz9BzOq9io"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cuda.enable_flash_sdp(True)"
      ],
      "metadata": {
        "id": "lyAEOdWx1QwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()\n",
        "\n",
        "torch.save(model.state_dict(), os.path.join(Config.checkpoint_dir, 'final_model.pth'))"
      ],
      "metadata": {
        "id": "X9P6wuc5rfUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train')\n",
        "plt.plot(np.linspace(0, len(train_losses), len(val_losses)), val_losses, label='Validation')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train')\n",
        "plt.plot(np.linspace(0, len(train_accuracies), len(val_accuracies)), val_accuracies, label='Validation')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig(os.path.join(Config.checkpoint_dir, 'training_curves.png'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w9dQskgnrg3x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}